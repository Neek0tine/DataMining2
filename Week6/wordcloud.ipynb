{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_type', 'url', 'date', 'content', 'renderedContent', 'id', 'user',\n",
       "       'replyCount', 'retweetCount', 'likeCount', 'quoteCount',\n",
       "       'conversationId', 'lang', 'source', 'sourceUrl', 'sourceLabel',\n",
       "       'outlinks', 'tcooutlinks', 'media', 'retweetedTweet', 'quotedTweet',\n",
       "       'inReplyToTweetId', 'inReplyToUser', 'mentionedUsers', 'coordinates',\n",
       "       'place', 'hashtags', 'cashtags'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('esteh.json', lines=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Missing the same person, everyday\\n\\n#estehind...</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#luhutbinsarpanjaitan #AirlanggaHartarto #Joko...</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jualan itu kudu siap dikritik, bukan ngasih so...</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kisruh Es Teh Indonesia, ini kata Ernest Praka...</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fak estehendonesa !\\nMy hommies drink VONTEA (...</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>Warga Bekaseee jangan lupa hari ini yaaa.... üòç...</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>Ada #kopiindonesia, ada #estehindonesia luar b...</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>Sekopi kopinya kamu pasti minum es teh juga #e...</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>Sekopi-kopinya kamu. Es teh manis adalah kita ...</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>Once more congratulation beb @jodiebianca atas...</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'us...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>623 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               content  \\\n",
       "0    Missing the same person, everyday\\n\\n#estehind...   \n",
       "1    #luhutbinsarpanjaitan #AirlanggaHartarto #Joko...   \n",
       "2    Jualan itu kudu siap dikritik, bukan ngasih so...   \n",
       "3    Kisruh Es Teh Indonesia, ini kata Ernest Praka...   \n",
       "4    Fak estehendonesa !\\nMy hommies drink VONTEA (...   \n",
       "..                                                 ...   \n",
       "618  Warga Bekaseee jangan lupa hari ini yaaa.... üòç...   \n",
       "619  Ada #kopiindonesia, ada #estehindonesia luar b...   \n",
       "620  Sekopi kopinya kamu pasti minum es teh juga #e...   \n",
       "621  Sekopi-kopinya kamu. Es teh manis adalah kita ...   \n",
       "622  Once more congratulation beb @jodiebianca atas...   \n",
       "\n",
       "                                                  user  \n",
       "0    {'_type': 'snscrape.modules.twitter.User', 'us...  \n",
       "1    {'_type': 'snscrape.modules.twitter.User', 'us...  \n",
       "2    {'_type': 'snscrape.modules.twitter.User', 'us...  \n",
       "3    {'_type': 'snscrape.modules.twitter.User', 'us...  \n",
       "4    {'_type': 'snscrape.modules.twitter.User', 'us...  \n",
       "..                                                 ...  \n",
       "618  {'_type': 'snscrape.modules.twitter.User', 'us...  \n",
       "619  {'_type': 'snscrape.modules.twitter.User', 'us...  \n",
       "620  {'_type': 'snscrape.modules.twitter.User', 'us...  \n",
       "621  {'_type': 'snscrape.modules.twitter.User', 'us...  \n",
       "622  {'_type': 'snscrape.modules.twitter.User', 'us...  \n",
       "\n",
       "[623 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrap = df.drop(['_type', 'url', 'date', 'renderedContent', 'id',\n",
    "       'replyCount', 'retweetCount', 'likeCount', 'quoteCount',\n",
    "       'conversationId', 'lang', 'source', 'sourceUrl', 'sourceLabel',\n",
    "       'outlinks', 'tcooutlinks', 'media', 'retweetedTweet', 'quotedTweet',\n",
    "       'inReplyToTweetId', 'inReplyToUser', 'mentionedUsers', 'coordinates',\n",
    "       'place', 'hashtags', 'cashtags'], axis=1)\n",
    "scrap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Missing the same person, everyday\\n\\n#estehind...</td>\n",
       "      <td>@p_andalusia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#luhutbinsarpanjaitan #AirlanggaHartarto #Joko...</td>\n",
       "      <td>@TeknokratJawa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jualan itu kudu siap dikritik, bukan ngasih so...</td>\n",
       "      <td>@txtdrpengusaha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kisruh Es Teh Indonesia, ini kata Ernest Praka...</td>\n",
       "      <td>@Beritasatu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fak estehendonesa !\\nMy hommies drink VONTEA (...</td>\n",
       "      <td>@YudhaSKAlatiga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>Warga Bekaseee jangan lupa hari ini yaaa.... üòç...</td>\n",
       "      <td>@fatinfebri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>Ada #kopiindonesia, ada #estehindonesia luar b...</td>\n",
       "      <td>@baimjalu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>Sekopi kopinya kamu pasti minum es teh juga #e...</td>\n",
       "      <td>@kurniaw05441616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>Sekopi-kopinya kamu. Es teh manis adalah kita ...</td>\n",
       "      <td>@bundaorizaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>Once more congratulation beb @jodiebianca atas...</td>\n",
       "      <td>@rezaalle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>623 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               content              user\n",
       "0    Missing the same person, everyday\\n\\n#estehind...      @p_andalusia\n",
       "1    #luhutbinsarpanjaitan #AirlanggaHartarto #Joko...    @TeknokratJawa\n",
       "2    Jualan itu kudu siap dikritik, bukan ngasih so...   @txtdrpengusaha\n",
       "3    Kisruh Es Teh Indonesia, ini kata Ernest Praka...       @Beritasatu\n",
       "4    Fak estehendonesa !\\nMy hommies drink VONTEA (...   @YudhaSKAlatiga\n",
       "..                                                 ...               ...\n",
       "618  Warga Bekaseee jangan lupa hari ini yaaa.... üòç...       @fatinfebri\n",
       "619  Ada #kopiindonesia, ada #estehindonesia luar b...         @baimjalu\n",
       "620  Sekopi kopinya kamu pasti minum es teh juga #e...  @kurniaw05441616\n",
       "621  Sekopi-kopinya kamu. Es teh manis adalah kita ...      @bundaorizaa\n",
       "622  Once more congratulation beb @jodiebianca atas...         @rezaalle\n",
       "\n",
       "[623 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listuser = []\n",
    "for row in scrap['user']:\n",
    "    listuser.append('@'+row['username'])\n",
    "scrap['user'] = listuser\n",
    "scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Missing the same person, everyday\\n\\n#estehind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#luhutbinsarpanjaitan #AirlanggaHartarto #Joko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jualan itu kudu siap dikritik, bukan ngasih so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kisruh Es Teh Indonesia, ini kata Ernest Praka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fak estehendonesa !\\nMy hommies drink VONTEA (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>Warga Bekaseee jangan lupa hari ini yaaa.... üòç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>Ada #kopiindonesia, ada #estehindonesia luar b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>Sekopi kopinya kamu pasti minum es teh juga #e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>Sekopi-kopinya kamu. Es teh manis adalah kita ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>Once more congratulation beb @jodiebianca atas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>623 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               content\n",
       "0    Missing the same person, everyday\\n\\n#estehind...\n",
       "1    #luhutbinsarpanjaitan #AirlanggaHartarto #Joko...\n",
       "2    Jualan itu kudu siap dikritik, bukan ngasih so...\n",
       "3    Kisruh Es Teh Indonesia, ini kata Ernest Praka...\n",
       "4    Fak estehendonesa !\\nMy hommies drink VONTEA (...\n",
       "..                                                 ...\n",
       "618  Warga Bekaseee jangan lupa hari ini yaaa.... üòç...\n",
       "619  Ada #kopiindonesia, ada #estehindonesia luar b...\n",
       "620  Sekopi kopinya kamu pasti minum es teh juga #e...\n",
       "621  Sekopi-kopinya kamu. Es teh manis adalah kita ...\n",
       "622  Once more congratulation beb @jodiebianca atas...\n",
       "\n",
       "[623 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contentdf = scrap.copy()\n",
    "contentdf = contentdf.drop(['user'], axis=1)\n",
    "contentdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import index\n",
    "import re, string\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import demoji  \n",
    "\n",
    "# print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))\n",
    "\n",
    "\n",
    "def bersih_text(text): # first\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"@[A-Za-z0-9_]+\", \"\", text)  # Menghapus @<name> [mention twitter]\n",
    "    text = re.sub(\"#\\w+\", \"\", text)\n",
    "    text = re.sub(\"\\[.*?\\]\", \"\", text)\n",
    "    text = re.sub(\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "    text = re.sub(\"<.*?>+\", \"\", text)\n",
    "    text = re.sub(\"[%s]\" % re.escape(string.punctuation), \"\", text)\n",
    "    text = demoji.replace(text, \"\")\n",
    "    # text = re.sub('\\n', '', text)\n",
    "    text = re.sub(\"\\w*\\d\\w*\", \"\", text)\n",
    "    text = re.sub(\"\\d+\", \"\", text)\n",
    "    text = re.sub(\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "\n",
    "\n",
    "    # text = re.sub('\\n', '', text) jadi:\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "def tokennization(text): # second\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def stopword(text): # third\n",
    "    return [word for word in text if word not in StopWordRemoverFactory().get_stop_words()]\n",
    "\n",
    "alltext = []\n",
    "for index, value in contentdf.iterrows():\n",
    "    alltext.append(bersih_text(value[0]))\n",
    "\n",
    "alltext = \" \".join(alltext)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nlp_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [16], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnlp_id\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FreqDist\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nlp_id'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import nlp_id\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "from collections.abc import Sequence\n",
    "\n",
    "from nlp_id.stopword import StopWord\n",
    "from nlp_id.lemmatizer import Lemmatizer \n",
    "from nlp_id.tokenizer import Tokenizer \n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "tokenizer = Tokenizer() \n",
    "\n",
    "for column, content in df.iteritems():\n",
    "    for index, value in enumerate(df[column]):\n",
    "        df[column][index] = tokenizer.tokenize(df[column][index])\n",
    "\n",
    "df_token = df.copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e5bdc5626e3ee0b9f2cb17ad4f32f901df6de7438aedae0b4184927feb23b082"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
